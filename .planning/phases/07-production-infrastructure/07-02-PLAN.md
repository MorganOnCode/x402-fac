---
phase: 07-production-infrastructure
plan: 07-02
type: execute
wave: 1
depends_on: []
files_modified:
  - Dockerfile
  - .dockerignore
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Multi-stage Dockerfile produces a minimal production image"
    - "Production image runs as non-root user"
    - "No dev dependencies in production image"
    - "Docker Compose has a production profile with Redis auth"
    - "Production image builds and starts successfully"
    - ".dockerignore excludes secrets, tests, and dev artifacts"
  artifacts:
    - path: "Dockerfile"
      provides: "Multi-stage production Docker image for the facilitator"
      exports: []
    - path: ".dockerignore"
      provides: "Docker build context exclusion rules"
      exports: []
    - path: "docker-compose.yml"
      provides: "Updated compose with production profile"
      exports: []
  key_links:
    - from: "Dockerfile"
      to: "package.json"
      via: "Copies package.json and installs production deps"
      pattern: "pnpm install --prod|pnpm build"
    - from: "docker-compose.yml"
      to: "Dockerfile"
      via: "Production profile builds from Dockerfile"
      pattern: "profiles.*production|build.*context"
---

<objective>
Create a production-ready Docker setup: multi-stage Dockerfile (build → slim runtime), .dockerignore, and a production profile in docker-compose.yml with Redis authentication.

Purpose: The facilitator needs to be containerized for deployment. The existing docker-compose.yml only runs Redis and IPFS for development. This plan adds the ability to build and run the facilitator itself as a Docker container with production-grade configuration.

Output: `Dockerfile`, `.dockerignore`, updated `docker-compose.yml`
</objective>

<context>
@package.json
@tsup.config.ts
@src/index.ts
@src/config/index.ts
@docker-compose.yml
@.gitignore
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create .dockerignore</name>
  <files>.dockerignore</files>
  <action>
Create `.dockerignore` to minimize build context and prevent secrets from being copied:

```
node_modules
dist
coverage
.git
.github
.planning
.auditing
.obsidian
.claude
.husky
tests
config/config.json
config/cmds.bash
.env
.env.*
*.log
logs
.DS_Store
.vitest
*.md
!package.json
```

Key exclusions:
- `config/config.json` — secrets must never be in the image. Config is mounted at runtime.
- `node_modules` — reinstalled in the build stage for correct platform binaries.
- `tests`, `coverage`, `.planning`, `.auditing` — not needed in production.
- `.git` — large directory, not needed.
- `*.md` files excluded but `package.json` explicitly included (it's not .md but the `!` pattern prevents accidental exclusion by future rules).
  </action>
  <verify>
- `.dockerignore` exists
- `config/config.json` is excluded
- `node_modules` is excluded
- `tests` directory is excluded
  </verify>
  <done>.dockerignore created excluding secrets, tests, and dev artifacts.</done>
</task>

<task type="auto">
  <name>Task 2: Create multi-stage Dockerfile</name>
  <files>Dockerfile</files>
  <action>
Create a multi-stage Dockerfile:

```dockerfile
# Stage 1: Build
FROM node:20-alpine AS build

# Enable corepack for pnpm
RUN corepack enable

WORKDIR /app

# Copy package files first (layer caching)
COPY package.json pnpm-lock.yaml ./

# Install all dependencies (including dev for build)
RUN pnpm install --frozen-lockfile

# Copy source code
COPY src/ src/
COPY tsconfig.json tsconfig.build.json tsup.config.ts ./

# Build
RUN pnpm build

# Stage 2: Production
FROM node:20-alpine AS production

RUN corepack enable

WORKDIR /app

# Create non-root user
RUN addgroup -g 1001 -S appgroup && \
    adduser -S appuser -u 1001 -G appgroup

# Copy package files
COPY package.json pnpm-lock.yaml ./

# Install production dependencies only
RUN pnpm install --frozen-lockfile --prod

# Copy built output from build stage
COPY --from=build /app/dist ./dist

# Copy config example (actual config mounted at runtime)
COPY config/config.example.json ./config/config.example.json

# Create config directory for runtime mount
RUN mkdir -p config && chown appuser:appgroup config

# Switch to non-root user
USER appuser

# Expose default port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
  CMD wget --no-verbose --tries=1 --spider http://localhost:3000/health || exit 1

# Start the application
CMD ["node", "dist/index.js"]
```

Key design decisions:
- **Alpine base** — minimal image size (~180MB vs ~1GB for full node image).
- **Two stages** — build stage has dev deps (tsup, typescript), production stage has only runtime deps.
- **Non-root user** (appuser:1001) — security best practice, satisfies Phase 7 security check.
- **`pnpm install --frozen-lockfile --prod`** — no dev dependencies in production image, satisfies security check.
- **Config mounted at runtime** — `config/config.json` is never baked into the image. Operators mount it via Docker volume or bind mount.
- **HEALTHCHECK** — uses wget (available in Alpine) to probe /health endpoint.
- **Layer caching** — package files copied before source code so `pnpm install` is cached when only source changes.
- **`node dist/index.js`** instead of `pnpm start` — avoids pnpm overhead in production, runs the built bundle directly.

Note on pnpm and corepack: Node 20 Alpine includes corepack. `corepack enable` activates it, and pnpm version is derived from the `packageManager` field in `package.json`.
  </action>
  <verify>
- `Dockerfile` exists with two stages (build, production)
- Production stage uses non-root user
- Production stage runs `pnpm install --prod` (no dev deps)
- `config/config.json` is NOT copied into the image
- HEALTHCHECK directive present
- Build succeeds: `docker build -t x402-fac .`
  </verify>
  <done>Multi-stage Dockerfile created. Production image runs as non-root with no dev dependencies.</done>
</task>

<task type="auto">
  <name>Task 3: Add production profile to docker-compose.yml</name>
  <files>docker-compose.yml</files>
  <action>
Update `docker-compose.yml` to add a production profile alongside the existing development services.

The existing file has two services (ipfs, redis) for development. Add:

1. **A `facilitator` service** with the `production` profile
2. **A `redis-prod` service** with the `production` profile (Redis with auth)
3. Keep existing `ipfs` and `redis` services unchanged (they remain the default dev services)

Updated file structure:

```yaml
# Development and production Docker services
# Dev:  docker compose up (starts redis + ipfs for local development)
# Prod: docker compose --profile production up (starts facilitator + redis with auth)

services:
  # --- Development services (default, no profile needed) ---

  ipfs:
    image: ipfs/kubo:latest
    container_name: x402-ipfs
    ports:
      - "4001:4001"
      - "5001:5001"
      - "8080:8080"
    volumes:
      - ipfs_data:/data/ipfs
    environment:
      - IPFS_PATH=/data/ipfs
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: x402-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    restart: unless-stopped

  # --- Production services (--profile production) ---

  facilitator:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: x402-facilitator
    profiles:
      - production
    ports:
      - "3000:3000"
    volumes:
      - ./config/config.json:/app/config/config.json:ro
    depends_on:
      redis-prod:
        condition: service_healthy
    restart: unless-stopped

  redis-prod:
    image: redis:7-alpine
    container_name: x402-redis-prod
    profiles:
      - production
    ports:
      - "6380:6379"
    volumes:
      - redis_prod_data:/data
    command: redis-server --appendonly yes --requirepass "${REDIS_PASSWORD:-changeme}"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-changeme}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

volumes:
  ipfs_data:
  redis_data:
  redis_prod_data:
```

Key decisions:
- **Profiles** — `docker compose up` still works exactly as before (dev only). `docker compose --profile production up` starts the facilitator + Redis with auth.
- **Redis auth** — Production Redis uses `--requirepass` with password from `REDIS_PASSWORD` env var (defaults to `changeme` for testing, must be overridden in real deployment).
- **Config mount** — `config/config.json` bind-mounted read-only (`:ro`). Operator must create this file before starting.
- **Depends on healthy** — Facilitator waits for Redis health check before starting.
- **Port 6380** for production Redis — avoids conflict with dev Redis on 6379 if both profiles run.
- **No IPFS in production** — as specified in ROADMAP deliverables.
  </action>
  <verify>
- `docker compose config` validates (default profile)
- `docker compose --profile production config` validates
- Existing dev services unchanged
- Production Redis has `--requirepass`
- Facilitator has read-only config mount
- `redis_prod_data` volume defined
  </verify>
  <done>Docker Compose updated with production profile. Redis auth enforced, facilitator containerized.</done>
</task>

</tasks>

<verification>
- `docker build -t x402-fac .` succeeds
- Built image runs as non-root: `docker run --rm x402-fac whoami` → `appuser`
- No dev dependencies in image: `docker run --rm x402-fac ls node_modules/.package-lock.json` shows only production deps
- `docker compose config` validates (dev profile unchanged)
- `docker compose --profile production config` validates
- `.dockerignore` excludes `config/config.json`
</verification>

<success_criteria>
The facilitator can be built as a minimal Docker image and run in a production-like environment with Redis authentication. The image runs as non-root with no dev dependencies. Development workflow is unchanged — `docker compose up` still works as before.
</success_criteria>

<output>
After completion, create `.planning/phases/07-production-infrastructure/07-02-SUMMARY.md`
</output>
